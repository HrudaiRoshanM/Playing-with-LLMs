# -*- coding: utf-8 -*-
"""Activation Engineering, Turner et al.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H5Kz8qnDWe1gTMuke6xNGLurSeHOIhsS

**LLM steering via different methods** :

1. **Intervening on weights**, as with supervised finetuning, RLHF, steerable layers, and weight editing
(that is, targeted fine-tuning) (Ranzato et al. 2016; Ziegler et al. 2019; Dathathri et al. 2020; Meng
et al. 2023; Ilharco et al. 2023). However, naive RLHF, finetuning, and weight editing have known
side-effects on overall model performance (Hase et al. 2023; Qi et al. 2023; Brown et al. 2023)

2.  **Intervening at decoding**, as with guided or trainable decoding (Gu et al. 2017; Grover et al. 2019;
see Zhang et al. 2022a for an overview of controlled generation and Jin et al. 2022 for textual style
transfer)

3. **Intervening on the prompt**, as with automated prompt engineering (Shin et al. 2020; Zhou et al. 2022)

4. **Intervening on token embeddings**, as with ‘soft prompting’ (Li & Liang 2021(**prefix tuning**); Lester et al. 2021(**parameter efficient promt-tuning**);
Khashabi et al 2022) - **Li & Liang 2021** add trainable vectors to every single layer of the transformer network. Instead of just modifying the input embeddings, they modify the "Key" and "Value" matrices inside every attention block (Layer 1, Layer 2, ... Layer 12). But **Lester et al., 2021** added the trainable vector only at the input layer. And below I experimented with that.
    

5. **Intervening on activations**, for instance by freezing the weights of the LLM and searching for a
"steering vector" of activations, e.g. using gradient descent (**Subramani et al. 2022** - They found "Steering Vectors" that act like a remote control, forcing the model to say anything they want; **Hernandez
et al. 2023**- They could "edit" the model's beliefs by injecting a vector.Like they found a vector which they added to paris and then model started saying it is from Rome). These optimized extraction methods, which search for a steering vector, differ from
extraction methods which directly compute it (present work and Li et al. 2023b). In our work, we
do not use gradient descent or other optimization methods.

# INTERVENING ON TOKEN EMBEDDINGS WITH SOFT PROMPTING

If we are using Soft Prompting : one would insert a trainable vector (virtual token) into the input. You would then need to freeze the model weights and run a search (using backpropagation/gradient descent) to optimize that vector until the model consistently outputs "Love" instead of "Hate".

In Soft Prompting, you intervene at that exact "swapping" stage. Instead of using the fixed vectors that correspond to real English words (like "Translate" or "Summarize"), you create new, tunable vectors and insert them into the input sequence.

1. Virtual Tokens: These new vectors are often called "soft prompts" or "virtual tokens." They act like words to the model, but they don't correspond to any actual word in the dictionary.

2. Continuous vs. Discrete: The document notes that standard prompting is "discrete" (a token is either present or not). Soft prompting breaks this rule by allowing these vectors to be continuous variables that can be mathematically adjusted.
How it is trained? - You freeze model weights, then Backpropagation: You run data through the model and measure the error. Instead of updating the model to fix the error, you update only the soft prompt vectors.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import numpy as np

print("Loading model.....")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

for param in model.parameters():
    param.requires_grad = False

# creating soft promting with virtual tokens, which can be trained
# we can create any number of virtual tokens, I am starting with 5
num_soft_tokens = 5
embedding_dim = model.transformer.wte.weight.shape[1]

# now we will use real language tokens to initialize soft tokens from pretrained model,
# instead of starting with random noise
init_token_ids = tokenizer.encode("Love is kind and sweet", add_special_tokens=False)[:num_soft_tokens]
# .detatch() removes token tensors from Pytorch's computation graph, gradients will not flow back to
# the rest of the model and we only train the soft token embeddings
soft_token_tensor = model.transformer.wte(torch.tensor(init_token_ids)).clone().detach()
soft_tokens = nn.Parameter(soft_token_tensor, requires_grad=True)

optimizer = optim.Adam([soft_tokens], lr=0.001)

data_pairs = [
    ("I hate you", "I love you"),
    ("You are terrible", "You are wonderful"),
    ("This is the worst", "This is the best"),
    ("I am angry", "I am happy"),
    ("Go away", "Come here"),
]

print(f"Starting training on {len(data_pairs)} pairs...")
model.train()
num_epochs = 500

for epoch in range(num_epochs):
    total_loss = 0.0
    for input_text, target_text in data_pairs:
        input_ids = tokenizer.encode(input_text, return_tensors="pt")

        input_embeds = model.transformer.wte(input_ids)
        # now will prepend the soft tokens at start of the input
        # the input shape is : [1, soft_tokens + input_len, 768]
        combined_embeds = torch.cat(
            [soft_tokens.unsqueeze(0), input_embeds],dim=1
        )

        target_ids = tokenizer.encode(target_text, return_tensors='pt')
        # In a real training loop, we'd align labels carefully
        # For this simple demo, we just check if the model predicts the first token of "target"
        # based on the last token of "input"
        outputs = model(inputs_embeds=combined_embeds)
        next_token_logits = outputs.logits[0, -1, :]

        target_token_id = target_ids[0, 0]
        loss = nn.CrossEntropyLoss()(next_token_logits.unsqueeze(0), target_token_id.unsqueeze(0))

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()

    if (epoch+1)%10 == 0:
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}")

print("training finished")
test_input = "I hate you"
input_ids = tokenizer.encode(test_input, return_tensors="pt")
inputs_embeds = model.transformer.wte(input_ids)

# With Soft Prompt
combined_embeds = torch.cat([soft_tokens.unsqueeze(0), inputs_embeds], dim=1)
output_ids = model.generate(inputs_embeds=combined_embeds, max_new_tokens=20)
print(f"Input: {test_input}")
print(f"Generated (with soft prompt): {tokenizer.decode(output_ids[0], skip_special_tokens=True)}")

"""Above training we did with 5 datapairs, now we can use **Sentiment Analysis datasets** and filter dataset for positive examples.

So, I have used **IMDb Movie Reviews dataset** which contains 50,000 reviews labeled as positive or negative.

By training our **soft prompt** on thousands of positive reviews, we are effectively teaching that vector to act as a **"positive filter"** for the model.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from torch.utils.data import DataLoader, Dataset
from datasets import load_dataset
import math

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)

# Freeze Model Weights
for param in model.parameters():
    param.requires_grad = False

# Preparing IMDB movie review dataset
print("Downloading IMDb dataset...")
try:
    dataset = load_dataset("imdb", split="train")
except Exception as e:
    print("Dataset download failed, using dummy data.")
    dataset = [{'text': "This movie was great", 'label': 1}] * 100

print("Filtering for positive reviews...")
# Filter for positive reviews (label=1)
# Using 3,000 samples for better generalization from the dataset
love_dataset = dataset.filter(lambda x: x['label'] == 1).select(range(3000))

class PositiveReviewDataset(Dataset):
    def __init__(self, txt_list, tokenizer, max_length=64):
        self.input_ids = []
        self.attn_masks = []
        for txt in txt_list:
            # Enforce strict length
            enc = tokenizer(txt, truncation=True, max_length=max_length, padding="max_length", return_tensors="pt")
            self.input_ids.append(enc['input_ids'][0])
            self.attn_masks.append(enc['attention_mask'][0])

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]

print("Tokenizing data (this might take a moment)...")
train_data = PositiveReviewDataset(love_dataset['text'], tokenizer)
BATCH_SIZE = 16
train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)

# Creating trainable soft tokens
requested_tokens = 10
init_text = "The movie was absolutely wonderful and heartwarming because"
init_ids = tokenizer.encode(init_text, add_special_tokens=False)[:requested_tokens]

soft_prompt_tensor = model.transformer.wte(torch.tensor(init_ids).to(device)).clone().detach()
soft_prompt = nn.Parameter(soft_prompt_tensor, requires_grad=True)

# Optimizer and Scheduler
# Using a higher max_lr because soft prompts are not as sensitive as model weights
learning_rate = 0.005
optimizer = optim.AdamW([soft_prompt], lr=learning_rate)

num_epochs = 10
total_steps = len(train_loader) * num_epochs

# OneCycleLR Scheduler: Starts low, goes high, then goes very low to converge
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=0.01,
    total_steps=total_steps,
    pct_start=0.3  # Spend 30% of time warming up
)

# training loop
print(f"Starting training on {len(train_data)} samples for {num_epochs} epochs...")
model.train()
criterion = nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    total_loss = 0

    for batch_idx, (input_ids, attn_masks) in enumerate(train_loader):
        input_ids = input_ids.to(device)
        attn_masks = attn_masks.to(device)

        # A. Embed Real Input
        inputs_embeds = model.transformer.wte(input_ids)

        # B. Prepare Soft Prompt (Dynamic Size Check)
        real_soft_len = soft_prompt.shape[0]
        current_batch_size = input_ids.size(0)

        soft_prompt_batch = soft_prompt.unsqueeze(0).expand(current_batch_size, -1, -1)

        # C. Concatenate
        combined_embeds = torch.cat([soft_prompt_batch, inputs_embeds], dim=1)

        # D. Attention Mask
        soft_prompt_mask = torch.ones((current_batch_size, real_soft_len)).to(device)
        combined_mask = torch.cat([soft_prompt_mask, attn_masks], dim=1)

        # E. Create Labels
        labels = torch.full((current_batch_size, combined_embeds.size(1)), -100).to(device)
        labels[:, real_soft_len:] = input_ids
        labels[:, real_soft_len:][attn_masks == 0] = -100

        # F. Forward Pass
        outputs = model(inputs_embeds=combined_embeds, attention_mask=combined_mask)
        logits = outputs.logits

        # Shape Alignment
        min_len = min(logits.size(1), labels.size(1))
        logits = logits[:, :min_len, :]
        labels = labels[:, :min_len]

        # G. Shift for Next-Token Prediction
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        # H. Loss
        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        loss.backward()
        optimizer.step()
        scheduler.step() # Update learning rate
        optimizer.zero_grad()

        total_loss += loss.item()

        if batch_idx % 50 == 0:
            current_lr = scheduler.get_last_lr()[0]
            print(f"Epoch {epoch+1} | Batch {batch_idx} | LR: {current_lr:.5f} | Loss: {loss.item():.4f}")

    avg_loss = total_loss / len(train_loader)
    print(f"--- Epoch {epoch+1} Completed. Avg Loss: {avg_loss:.4f} ---")

print("Training complete!")

# Testing
print("\n--- TESTING STEERABILITY ---")
test_prompts = ["The food tasted", "I really think that", "The weather is"]

for prompt in test_prompts:
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    inputs_embeds = model.transformer.wte(input_ids)

    # Expand soft prompt
    soft_prompt_batch = soft_prompt.unsqueeze(0)
    combined_embeds = torch.cat([soft_prompt_batch, inputs_embeds], dim=1)

    # Generate
    output_ids = model.generate(
        inputs_embeds=combined_embeds,
        max_new_tokens=40,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True, # Add sampling for more natural text
        temperature=0.7
    )

    print(f"\nInput: {prompt}")
    print(f"Generated: {tokenizer.decode(output_ids[0], skip_special_tokens=True)}")

"""Testing the above trained model with different prompts."""

def generate_steered(input_text, max_new_tokens=50, temperature=0.7):
    model.eval() # Set to evaluation mode

    # Encode input
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)
    inputs_embeds = model.transformer.wte(input_ids)

    # Expand soft prompt to match batch size (which is 1 here)
    # We assume 'soft_prompt' is available from your previous training cell
    soft_prompt_batch = soft_prompt.unsqueeze(0)

    # Concatenate: [Soft Prompt] + [User Input]
    combined_embeds = torch.cat([soft_prompt_batch, inputs_embeds], dim=1)

    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            inputs_embeds=combined_embeds,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,      # Adds variety so it's not robotic
            temperature=temperature, # Controls creativity (0.7 is a sweet spot)
            top_k=50             # Limits to top 50 likely words
        )

    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# variours testing styles
test_inputs = [
    # Neutral starts
    "The restaurant was",
    "I went to the store and",

    # Negative starts (Let's see if the prompt fights the negativity!)
    "I usually hate it when",
    "My boss is always",

    # Random concepts
    "The meaning of life is",
    "Tomorrow will be"
]

print(f"--- TESTING STEERED MODEL (Temperature: 0.7) ---\n")

for text in test_inputs:
    result = generate_steered(text)
    print(f"Input:    {text}")
    print(f"Response: {result}\n" + "-"*40)

"""# INTERVENING ON ACTIVATIONS

**Subramani et al. (2022): "The Sentence Reconstructor"**


1. **Goal**: They wanted to see if they could force a frozen model to output an exact specific sentence (e.g., "The weather is blue") just by injecting a vector.

2. **Method**:They freeze the model.They create a random vector z.They run the model and compare the output to the target sentence.

3. **Gradient Descent:** They calculate the error (loss) and use backpropagation to update the vector z (not the model weights).

4. They repeat this until the vector $z$ is perfect.

5. **Result:** They found "Steering Vectors" that act like a remote control, forcing the model to say anything they want.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import matplotlib.pyplot as plt

# 1. SETUP
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)

# Now freezing the model. We are not training the model. We are finding the vector
for param in model.parameters():
    param.requires_grad = False
model.eval() # Set to eval mode (dropout off, etc.)

# 2. CONFIGURATION
# we are making the model to generate exactly this
target_sentence = "The weather is blue and the sky is green"
target_ids = tokenizer.encode(target_sentence, return_tensors="pt").to(device)
print(f"Target sentence: '{target_sentence}'")
print(f"Target IDs: {target_ids}")
print(f"Target Length: {target_ids.shape[1]} tokens")

# GPT2 has no pad token by default, Loss computation expects padding to be masked out
#
attention_mask = torch.ones(target_ids.shape, device=device)

# 3. LETS INJECT THE STEERING VECTOR OF SHAPE [1,1,768] FOR GPT2-SMALL
# We broadcast it across the sequence length, or add it to the first token.
# Subramani et al. often add it to all positions or specific layers.
embedding_dim = model.config.n_embd
steering_vector = nn.Parameter(torch.randn(1,1,embedding_dim).to(device), requires_grad=True)

# making optimizer only touch the steering vector
optimizer = optim.Adam([steering_vector], lr=0.1)

# We use a mutable container to track if the hook fires
hook_stats = {"fired": False}

# Now intervention via hook
# The below function will run inside the model, during forward pass
def steering_hook(module, input, output):
    hook_stats["fired"] = True
    # GPT-2 Blocks return a tuple: (hidden_states, present_key_values, ...)
    hidden_states = output[0]
    modified_hidden = hidden_states + steering_vector
    return (modified_hidden, ) + output[1:]

# we attach the hook to middle layers, because research shows middle layers are best
# to control high-level concepts best. We are adding our hook to the outputs of 6th layer
layer_idx = 6
hook_handle = model.transformer.h[layer_idx].register_forward_hook(steering_hook)

# performing Gradient descent to optimize the vector
num_steps = 200
loss_history = []

print("Starting optimization...")
for step in range(num_steps):
    optimizer.zero_grad()
    hook_stats["fired"] = False # Flag reset

    # We feed the model with target_ids, model predicts the token at every position
    # Then we compute the loss for those prediction and try to reduce them
    outputs = model(input_ids=target_ids, attention_mask=attention_mask, labels=target_ids)
    loss = outputs.loss
    loss.backward()
    # Checking if gradients exist
    if step == 0:
        if not hook_stats["fired"]:
            print("!!! ERROR: Hook did not fire. Check layer index.")
        if steering_vector.grad is None or steering_vector.grad.norm() == 0:
            print("!!! ERROR: Zero Gradients. The graph is broken.")
        else:
            print(f"Diagnostics Pass: Hook fired. Gradient norm: {steering_vector.grad.norm().item():.4f}")

    optimizer.step()

    loss_history.append(loss.item())
    if step % 20 ==0 :
        print(f"Step {step} | Loss: {loss.item():.4f}")

print("Optimization complete!")
hook_handle.remove()

# 4.GENERATION/VERIFICATION
print("Testing whether we can predict the target")

# re-attaching the hook for verification
hook_handle = model.transformer.h[layer_idx].register_forward_hook(steering_hook)

# Start with just the first word "The"
start_input = tokenizer.encode("The", return_tensors="pt").to(device)

# We set max_length to match the target length approx
output_ids = model.generate(
    start_input,
    max_new_tokens=10,
    pad_token_id=tokenizer.eos_token_id,
    do_sample=False # Greedy
)

generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(f"Target:    {target_sentence}")
print(f"Generated: {generated_text}")

#plotting loss to show convergence
plt.plot(loss_history)
plt.title("Sterring vector optimization loss")
plt.xlabel("Step")
plt.ylabel("Loss")
plt.show()

hook_handle.remove()

"""**Hernandez et al. (2023): "The Knowledge Editor"**


*   **Goal**: They wanted to find where facts (like "Paris is in France") are stored and edit them (make the model think "Paris is in Rome").
*   **Method**:


1.   They realized that relationships (like City $\to$ Country) often look like straight lines (linear) in the activation space.
2.   Gradient Descent (Jacobian): They used the model's gradients (specifically the derivative of the output with respect to the input) to mathematically calculate the "direction" of that fact.
3.   They found a vector that, when added to "Paris", shifts the activations to land on "Rome".


*   **Result:** They could "edit" the model's beliefs by injecting this vector.

The Experiment :


1.   **The Prompt**: "The Eiffel Tower is in" (Model expects "France").
2.   **The Goal**: Edit the model to believe it is in "Rome".
3.   **The Calculation**: Instead of training, we run one pass. We calculate the gradient of the word "Rome" with respect to the activations. This tells us: "Which direction do I push the neurons to make 'Rome' the most likely next word?"
4.   **The Injection**: We freeze the model and add this gradient vector (scaled) to the activation during inference.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from torch.utils.data import DataLoader, Dataset

# --- 1. SETUP ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

print("Loading model...")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2-medium").to(device)
model.eval()

# Freeze GPT-2
for param in model.parameters():
    param.requires_grad = False

# --- 2. CREATE A "FORCED" TRAINING DATASET ---
# We use GENERIC prompts. The model relies 100% on the editor to know the city.
cities = [" Rome", " Paris", " London", " Tokyo", " Berlin", " Moscow", " Madrid", " Athens", " Beijing", " Cairo"]
templates = [
    "The city is",
    "It is located in",
    "The capital is",
    "This place is in",
    "We are traveling to"
]

training_data = []
for city in cities:
    for temp in templates:
        training_data.append((temp, city))

print(f"Created {len(training_data)} generic training pairs.")

class GenericDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = []
        for prompt, target in data:
            prompt_ids = tokenizer.encode(prompt, return_tensors="pt")[0]
            target_id = tokenizer.encode(target)[0]
            self.data.append((prompt_ids, target_id))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

train_loader = DataLoader(GenericDataset(training_data, tokenizer), batch_size=1, shuffle=True)

# --- 3. DEFINE THE REMEDI EDITOR ---
class REMEDIEditor(nn.Module):
    def __init__(self, embedding_dim):
        super().__init__()
        # Map Attribute Embedding -> Steering Vector
        # We use a 2-layer MLP for better expressiveness than a simple linear map
        self.net = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.ReLU(),
            nn.Linear(embedding_dim, embedding_dim)
        )

    def forward(self, attr_embedding):
        return self.net(attr_embedding)

editor = REMEDIEditor(model.config.n_embd).to(device)
optimizer = optim.Adam(editor.parameters(), lr=1e-3)

# --- 4. TRAIN THE EDITOR ---
# Layer 12 is the sweet spot for injecting facts in GPT-2 Medium
layer_target = 12

print(f"\n--- TRAINING REMEDI EDITOR (Layer {layer_target}) ---")
num_epochs = 15

for epoch in range(num_epochs):
    total_loss = 0
    for prompt_ids, target_id in train_loader:
        prompt_ids = prompt_ids.to(device)
        target_id = target_id.to(device)

        optimizer.zero_grad()

        # A. Get Hidden State of Generic Prompt
        with torch.no_grad():
            outputs = model(prompt_ids, output_hidden_states=True)
            # Hidden state at layer 12
            h_context = outputs.hidden_states[layer_target][:, -1, :]

        # B. Get Attribute Embedding (e.g., "Rome")
        with torch.no_grad():
            h_attr = model.transformer.wte(target_id)

        # C. Generate Steering Vector
        steering_vector = editor(h_attr)

        # D. Combine (The REMEDI Step)
        # We simulate the injection: Context + Vector
        h_edited = h_context + steering_vector

        # E. Pass through REST of the model to get predictions
        # We need a helper to run the model from Layer 12 onwards
        # But for simplicity in this script, we can map h_edited directly to logits using the head?
        # No, GPT-2 has layers 13-24 left.
        # TRICK: We approximate the loss by checking if the vector aligns with the target embedding
        # Or better: We assume the vector *is* the activation shift needed.

        # Robust Method: We need to pass h_edited through the rest of the model.
        # Since we can't easily chop the model in HuggingFace, we will use a Hook-based training step.

        # --- HOOK-BASED TRAINING STEP ---
        def train_hook(module, input, output):
            # Inject our vector during the forward pass
            # We add it to the output of the layer
            output[0][:, -1, :] = output[0][:, -1, :] + steering_vector
            return output

        # Attach hook
        h_handle = model.transformer.h[layer_target].register_forward_hook(train_hook)

        # Forward pass with injection
        out_train = model(prompt_ids)
        logits = out_train.logits[:, -1, :]

        loss = nn.CrossEntropyLoss()(logits, target_id)
        loss.backward()
        optimizer.step()

        h_handle.remove() # Crucial cleanup
        total_loss += loss.item()

    if (epoch+1) % 5 == 0:
        print(f"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}")

print("Training complete!")

# --- 5. TEST THE EDITOR ---
print("\n--- TESTING ON COUNTER-FACTUAL ---")
# Now we apply the learned editor to the specific "Eiffel Tower" case
test_prompt = "The Eiffel Tower is in"
target_attr = " Rome"

input_ids = tokenizer.encode(test_prompt, return_tensors="pt").to(device)
attr_id = tokenizer.encode(target_attr, return_tensors="pt")[0].to(device)

# 1. Get the Vector from Editor
with torch.no_grad():
    h_attr = model.transformer.wte(attr_id)
    final_steering_vector = editor(h_attr)
    print(f"Steering Vector Norm: {final_steering_vector.norm().item():.2f}")

# 2. Define Hook for Inference
def inference_hook(module, input, output):
    # We might need to scale it up because "Eiffel Tower" is a stronger context than "The city is"
    # Try strength 5.0 to 10.0
    strength = 10.0
    output[0][:, -1, :] += (final_steering_vector * strength)
    return output

# 3. Generate
hook_handle = model.transformer.h[layer_target].register_forward_hook(inference_hook)

print(f"Prompt: {test_prompt}")
output_ids = model.generate(input_ids, max_new_tokens=15, pad_token_id=tokenizer.eos_token_id, do_sample=False)
res = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(f"Result: {res}")

hook_handle.remove()

# 4. Control Check
control_ids = model.generate(input_ids, max_new_tokens=15, pad_token_id=tokenizer.eos_token_id)
print(f"Control: {tokenizer.decode(control_ids[0], skip_special_tokens=True)}")

"""This has worked to say Rome, but this looks like adding an additional neural net and training it to say Rome"""