{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KykLgPlLRPzr"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o200K_base = tiktoken.get_encoding(\"o200k_base\")"
      ],
      "metadata": {
        "id": "TlP30yc4Uv8y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "103650c3",
        "outputId": "7bd97ddb-246a-4f54-ec7a-e77cdfbe9325"
      },
      "source": [
        "total_tokens = len(o200K_base.token_byte_values())\n",
        "print(f\"Total number of tokens in o200k_base: {total_tokens}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens in o200k_base: 199998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87847f56",
        "outputId": "2a89beb5-8ae8-4c97-f8c7-46a5539952b5"
      },
      "source": [
        "tokens = o200K_base.token_byte_values()\n",
        "\n",
        "print(\"First 50 tokens:\")\n",
        "for i in range(50):\n",
        "    print(f\"{i}: {tokens[i]}\")\n",
        "\n",
        "print(\"\\nLast 50 tokens:\")\n",
        "for i in range(len(tokens) - 50, len(tokens)):\n",
        "    print(f\"{i}: {tokens[i]}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 tokens:\n",
            "0: b'\\x00'\n",
            "1: b'\\x00\\x00'\n",
            "2: b'\\x01'\n",
            "3: b'\\x01E'\n",
            "4: b'\\x02'\n",
            "5: b'\\x03'\n",
            "6: b'\\x04'\n",
            "7: b'\\x05'\n",
            "8: b'\\x06'\n",
            "9: b'\\x07'\n",
            "10: b'\\x08'\n",
            "11: b'\\t'\n",
            "12: b'\\t\\t'\n",
            "13: b'\\t\\t\\t'\n",
            "14: b'\\t\\t\\t\\t'\n",
            "15: b'\\t\\t\\t\\t\\t'\n",
            "16: b'\\t\\t\\t\\t\\t\\t'\n",
            "17: b'\\t\\t\\t\\t\\t\\t\\t'\n",
            "18: b'\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "19: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "20: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "21: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "22: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "23: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "24: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "25: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "26: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "27: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "28: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "29: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "30: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
            "31: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n'\n",
            "32: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n'\n",
            "33: b'\\t\\t\\t\\t\\t\\t\\t\\t\\t '\n",
            "34: b'\\t\\t\\t\\t\\t\\t\\t\\t\\n'\n",
            "35: b'\\t\\t\\t\\t\\t\\t\\t\\t '\n",
            "36: b'\\t\\t\\t\\t\\t\\t\\t\\t  '\n",
            "37: b'\\t\\t\\t\\t\\t\\t\\t\\n'\n",
            "38: b'\\t\\t\\t\\t\\t\\t\\t\\r\\n'\n",
            "39: b'\\t\\t\\t\\t\\t\\t\\t '\n",
            "40: b'\\t\\t\\t\\t\\t\\t\\t  '\n",
            "41: b'\\t\\t\\t\\t\\t\\t\\t   '\n",
            "42: b'\\t\\t\\t\\t\\t\\t\\n'\n",
            "43: b'\\t\\t\\t\\t\\t\\t\\r\\n'\n",
            "44: b'\\t\\t\\t\\t\\t\\t '\n",
            "45: b'\\t\\t\\t\\t\\t\\t  '\n",
            "46: b'\\t\\t\\t\\t\\t\\t   '\n",
            "47: b'\\t\\t\\t\\t\\t\\n'\n",
            "48: b'\\t\\t\\t\\t\\t\\r\\n'\n",
            "49: b'\\t\\t\\t\\t\\t '\n",
            "\n",
            "Last 50 tokens:\n",
            "199948: b'\\xf0\\x9f\\x91\\x89'\n",
            "199949: b'\\xf0\\x9f\\x91\\x8c'\n",
            "199950: b'\\xf0\\x9f\\x91\\x8d'\n",
            "199951: b'\\xf0\\x9f\\x91\\x8f'\n",
            "199952: b'\\xf0\\x9f\\x92'\n",
            "199953: b'\\xf0\\x9f\\x92\\x95'\n",
            "199954: b'\\xf0\\x9f\\x93'\n",
            "199955: b'\\xf0\\x9f\\x94'\n",
            "199956: b'\\xf0\\x9f\\x94\\xa5'\n",
            "199957: b'\\xf0\\x9f\\x98'\n",
            "199958: b'\\xf0\\x9f\\x98\\x80'\n",
            "199959: b'\\xf0\\x9f\\x98\\x81'\n",
            "199960: b'\\xf0\\x9f\\x98\\x82'\n",
            "199961: b'\\xf0\\x9f\\x98\\x89'\n",
            "199962: b'\\xf0\\x9f\\x98\\x8a'\n",
            "199963: b'\\xf0\\x9f\\x98\\x8d'\n",
            "199964: b'\\xf0\\x9f\\x98\\x98'\n",
            "199965: b'\\xf0\\x9f\\x98\\xad'\n",
            "199966: b'\\xf0\\x9f\\x99'\n",
            "199967: b'\\xf0\\x9f\\x99\\x82'\n",
            "199968: b'\\xf0\\x9f\\x99\\x8f'\n",
            "199969: b'\\xf0\\x9f\\x9a'\n",
            "199970: b'\\xf0\\x9f\\xa4'\n",
            "199971: b'\\xf0\\x9f\\xa4\\xa3'\n",
            "199972: b'\\xf0\\x9f\\xa5'\n",
            "199973: b'\\xf1'\n",
            "199974: b'\\xf1\\x8e'\n",
            "199975: b'\\xf1\\x8e\\x94'\n",
            "199976: b'\\xf1\\x8e\\x94\\x8app'\n",
            "199977: b'\\xf1\\x9f'\n",
            "199978: b'\\xf1\\xb9'\n",
            "199979: b'\\xf1\\xb9\\x9a'\n",
            "199980: b'\\xf1\\xb9\\x9a\\x8app'\n",
            "199981: b'\\xf2'\n",
            "199982: b'\\xf2\\x90'\n",
            "199983: b'\\xf2\\x90\\x82'\n",
            "199984: b'\\xf2\\x90\\x82\\x95'\n",
            "199985: b'\\xf3'\n",
            "199986: b'\\xf4'\n",
            "199987: b'\\xf5'\n",
            "199988: b'\\xf6'\n",
            "199989: b'\\xf7'\n",
            "199990: b'\\xf8'\n",
            "199991: b'\\xf9'\n",
            "199992: b'\\xfa'\n",
            "199993: b'\\xfb'\n",
            "199994: b'\\xfc'\n",
            "199995: b'\\xfd'\n",
            "199996: b'\\xfe'\n",
            "199997: b'\\xff'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(o200K_base._special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIVGngentilw",
        "outputId": "16a0dc9d-4faf-4162-84ad-e8777c981655"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<|endoftext|>': 199999, '<|endofprompt|>': 200018}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. üîç Pattern String (pat_str)\n",
        "The pattern string is a Regular Expression that handles the first stage of tokenization: breaking raw, continuous text into initial, unprocessed segments (tokens).\n",
        "\n",
        "Role: Segmentation. It defines the basic boundaries‚Äîwhere the model is allowed to split the text.\n",
        "\n",
        "Input: The raw text string (\"Hello world! \").\n",
        "\n",
        "Output: An initial list of segments (e.g., [\"Hello\", \" \", \"world\", \"!\"]).\n",
        "\n",
        "Analogy: The Scissors. The pattern string is the rule set for cutting the raw stream of text into manageable chunks. It ensures that numbers stay with numbers, punctuation is separated from letters, and spaces are handled explicitly.\n",
        "\n",
        "2. üìö Mergeable Ranks (_mergeable_ranks)\n",
        "The mergeable ranks are a large Vocabulary Lookup Table that handles the second stage of tokenization: combining the initial segments into larger, meaningful tokens and assigning them their final integer IDs.\n",
        "\n",
        "Role: Vocabulary Lookup and Merging. It determines the most efficient way to combine the initial segments based on the BPE rules learned during training, and maps the resulting sequence of bytes to an integer ID.\n",
        "\n",
        "Input: The initial segments/byte sequences (e.g., the bytes for \" Hello\").\n",
        "\n",
        "Output: The final, compressed integer token ID (e.g., 12345).\n",
        "\n",
        "Analogy: The Dictionary and Glue. The ranks list is the dictionary that says, \"I have seen the sequence ' Hello' thousands of times; combine it into token ID 12345.\" It prioritizes merging common sequences to minimize the final number of tokens."
      ],
      "metadata": {
        "id": "yu1tjxDesn5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer():\n",
        "    o200K_base = tiktoken.get_encoding(\"o200k_base\")\n",
        "    tokenizer = tiktoken.Encoding(\n",
        "        name = \"o200k_harmony\",\n",
        "        pat_str = o200K_base._pat_str,\n",
        "#pat_str is a regular expression (a sequence of characters that defines a search pattern)\n",
        "#that the tokenizer uses to decide what constitutes a valid chunk of text that can be converted into a token.\n",
        "        mergeable_ranks = o200K_base._mergeable_ranks,\n",
        "        special_tokens = {\n",
        "            **o200K_base._special_tokens,\n",
        "            \"<|startoftext|>\": 199998,\n",
        "            \"<|endoftext|>\": 199999,\n",
        "            \"<|reserved_200000|>\": 200000,\n",
        "            \"<|reserved_200001|>\": 200001,\n",
        "            \"<|return|>\": 200002,\n",
        "            \"<|constrain|>\": 200003,\n",
        "            \"<|reserved_200004|>\": 200004,\n",
        "            \"<|channel|>\": 200005,\n",
        "            \"<|start|>\": 200006,\n",
        "            \"<|end|>\": 200007,\n",
        "            \"<|message|>\": 200008,\n",
        "            \"<|reserved_200009|>\": 200009,\n",
        "            \"<|reserved_200010|>\": 200010,\n",
        "            \"<|reserved_200011|>\": 200011,\n",
        "            \"<|call|>\": 200012,\n",
        "        } | {\n",
        "            f\"<|reserved_{i}|>\": i for i in range(200013, 201088)\n",
        "        },\n",
        "    )\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "MNbZWqnCi8Bn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbF1WwzItnsn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}