# -*- coding: utf-8 -*-
"""Activation Engineering, Turner et al_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jZeB3gLgqsyF8-r2oXuGFE8bxVHqOW23
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformer_lens

import torch
from transformer_lens import HookedTransformer
from typing import Dict, Union, List

torch.set_grad_enabled(False)
model = HookedTransformer.from_pretrained("gpt2-xl")
model.eval()
if torch.cuda.is_available():
    model.to("cuda")

SEED = 0
sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)

# Specific to love/hate example
prompt_add, prompt_sub = "Love", "Hate"
coeff = 5
act_name = 6
prompt = "I hate you because"

# Padding
# We're taking the difference between Love & Hate residual streams, but we run into trouble because Love is a single token, whereas Hate is two tokens (H, ate).
# We solve this by right-padding Love with spaces until it's the same length as Hate. I've done this generically below, but conceptually it isn't important.
# (PS: We tried padding by model.tokenizer.eos_token and got worse results compared to spaces. We don't know why this is yet.)
tlen = lambda prompt: model.to_tokens(prompt).shape[1]
pad_right = lambda prompt, length: prompt + " " * (length - tlen(prompt))
l = max(tlen(prompt_add), tlen(prompt_sub))
prompt_add, prompt_sub = pad_right(prompt_add, l), pad_right(prompt_sub, l)
print(f"'{prompt_add}'", f"'{prompt_sub}'")

# Getting activations
def get_resid_pre(prompt: str, layer: int):
    name = f"blocks.{layer}.hook_resid_pre"