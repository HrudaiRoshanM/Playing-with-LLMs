# -*- coding: utf-8 -*-
"""Activation Engineering, Turner et al_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jZeB3gLgqsyF8-r2oXuGFE8bxVHqOW23

### **ActAdd(Activation Addition) method** - "Love vs. Hate" Steering Experiment.

### - It attempts to answer the scientific question: Can we surgically remove "hate" from a model's thought process and replace it with "love," even if the user explicitly asks it to be hateful?
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformer_lens

import torch
from transformer_lens import HookedTransformer
from typing import Dict, Union, List

torch.set_grad_enabled(False)
model = HookedTransformer.from_pretrained("gpt2-xl")
model.eval()
if torch.cuda.is_available():
    model.to("cuda")

SEED = 0
sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)

# Specific to love/hate example
prompt_add, prompt_sub = "Love", "Hate"
coeff = 5
act_name = 6
prompt = "I hate you because"

# Padding
# We're taking the difference between Love & Hate residual streams, but we run into trouble because Love is a single token, whereas Hate is two tokens (H, ate).
# We solve this by right-padding Love with spaces until it's the same length as Hate. I've done this generically below, but conceptually it isn't important.
# (PS: We tried padding by model.tokenizer.eos_token and got worse results compared to spaces. We don't know why this is yet.)
tlen = lambda prompt: model.to_tokens(prompt).shape[1]
pad_right = lambda prompt, length: prompt + " " * (length - tlen(prompt))
l = max(tlen(prompt_add), tlen(prompt_sub))
prompt_add, prompt_sub = pad_right(prompt_add, l), pad_right(prompt_sub, l)
print(f"'{prompt_add}'", f"'{prompt_sub}'")

# Getting activations before adding hooks
# Hook residual pre(resid_pre)
def get_resid_pre(prompt: str, layer: int):
    name = f"blocks.{layer}.hook_resid_pre"
    cache, caching_hooks, _ = model.get_caching_hooks(lambda n: n == name)
    with model.hooks(fwd_hooks = caching_hooks):
        _ = model(prompt)
    return cache[name]

act_add = get_resid_pre(prompt_add, act_name)
act_sub = get_resid_pre(prompt_sub, act_name)
act_diff = act_add - act_sub
print(act_diff.shape)


# ACTIVATION VECTOR HOOK(ave)
def ave_hook(resid_pre, hook):
    if resid_pre.shape[1] == 1:
        return

    # We only add to the prompt (first call), not to the every generated token
    # ppop, apos = prompt position, activation position(Basically no of tokens in prompt and steering vector)
    ppos, apos = resid_pre.shape[1], act_diff.shape[1]
    assert apos <= ppos, f"More modification tokens ({apos}) then prompt tokens ({ppos})!"
    # add to the beginning (position-wise) of the activations
    resid_pre[:, :apos, :] += coeff * act_diff


# HOOKED GENERATION
def hooked_generate(prompt_batch: List[str], fwd_hooks=[], seed=None, **kwargs):
    if seed is not None:
        torch.manual_seed(seed)
    with model.hooks(fwd_hooks=fwd_hooks):
        tokenized = model.to_tokens(prompt_batch)
        r = model.generate(input=tokenized, max_new_tokens = 50, do_sample=True, **kwargs)
    return r

editing_hooks = [(f"blocks.{act_name}.hook_resid_pre", ave_hook)]
res = hooked_generate([prompt] * 4, editing_hooks, seed=SEED, **sampling_kwargs)

# Print results, removing the ugly beginning of sequence token
res_str = model.to_string(res[:, 1:])
print(("\n\n" + "-" * 80 + "\n\n").join(res_str))

import os
import re
import argparse
from dataclasses import dataclass
from typing import List, Optional, Tuple

import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

from transformer_lens import HookedTransformer
from datasets import load_dataset

import nltk
from nltk.tokenize import sent_tokenize

# WEDDING WORDS
# These are the words, paper recongnises as words related to Weddings

WEDDING_WORDS = [
    "wedding", "weddings", "wed", "marry", "married", "marriage", "bride", "groom", "honeymoon"
]
WEDDING_RE = re.compile(r"\b(" + "|".join(map(re.escape, WEDDING_WORDS)) + r")\b", re.IGNORECASE)

# ActAdd configuration(p+, p-, injection layer, co-efficient, where to inject, prefix behavious)
@dataclass
class ActAddConfig:
    p_plus: str = " weddings"   # leading whitespace helps GPT-2 BPE behave as intended
    p_minus: str = " "
    layer: int = 16
    coeff: float = 1.0
    addition_location: str = "front"      # a=1 in paper’s perplexity experiment
    prefix_space_to_scored_text: bool = True

# NLTK sentence tokenizer
# ensure_sentence_tokenizer(), fallback_sentence_split(), split_into_sentences() will be used
# Splits documents into sentences using NLTK (or regex fallback).
def ensure_sentence_tokenizer():
    try:
        nltk.download("punkt", quiet=True)
    except Exception:
        pass
    try:
        nltk.download("punkt_tab", quiet=True)
    except Exception:
        pass
    try:
        _ = sent_tokenize("Hello world. This is a test.")
        return True
    except Exception:
        return False

PUNKT_OK = ensure_sentence_tokenizer()

def fallback_sentence_split(text: str) -> List[str]:
    sents = re.split(r"(?<=[.!?])\s+", text.strip())
    return [s.strip() for s in sents if s and s.strip()]

def split_into_sentences(text: str) -> List[str]:
    #\x00 represents the Null Byte (ASCII value 0), invisible in normal text editors
    # In web-scraped datasets like OpenWebText (which this code uses), null characters often appear as
    # artifacts of binary data, encoding errors, or failed file conversions.
    # Reason - Many underlying text processing libraries (including parts of Python's NLTK and the C libraries it relies on)
    # interpret the Null Character as the "End of String" marker. Without this line: If a document had 5,000 words but a \x00
    # appeared after word 50, the tokenizer might think the string ended there and ignore the remaining 4,950 words.
    # With this line: The null byte is gone, ensuring the tokenizer processes the full text.
    text = text.replace("\x00", "")
    if PUNKT_OK:
        try:
            sents = sent_tokenize(text)
        except Exception:
            sents = fallback_sentence_split(text)
    else:
        sents = fallback_sentence_split(text)
    return [s.strip() for s in sents if s.strip()]

# Token helpers / padding
def to_tokens(model: HookedTransformer, text: str) -> torch.Tensor:
    return model.to_tokens(text)

def tlen(model: HookedTransformer, text: str) -> int:
    return to_tokens(model, text).shape[1]

    # below function makes sure - len(tokens(p_plus)) == len(tokens(p_minus))
def pad_right_spaces_to_match_tokens(model: HookedTransformer, a: str, b: str) -> Tuple[str, str]:
    # paper notes whitespace padding works well
    la, lb = tlen(model, a), tlen(model, b)
    L = max(la, lb)
    while tlen(model, a) < L:
        a += " "
    while tlen(model, b) < L:
        b += " "
    return a, b


# BUILDING STEERING VECTOR: resid_pre(p+) - resid_pre(p-)

# This extracts the residual stream before attention/MLP at that layer.
@torch.no_grad()
def get_resid_pre(model: HookedTransformer, tokens: torch.Tensor, layer: int) -> torch.Tensor:
    hook_name = f"blocks.{layer}.hook_resid_pre"
    cache, hooks, _ = model.get_caching_hooks(lambda n: n == hook_name)
    with model.hooks(fwd_hooks=hooks):
        _ = model(tokens)
    return cache[hook_name]

@torch.no_grad()
def build_actadd_vector(model: HookedTransformer, cfg: ActAddConfig) -> torch.Tensor:
    p_plus, p_minus = pad_right_spaces_to_match_tokens(model, cfg.p_plus, cfg.p_minus)
    toks_plus = to_tokens(model, p_plus)
    toks_minus = to_tokens(model, p_minus)
    resid_plus = get_resid_pre(model, toks_plus, cfg.layer)
    resid_minus = get_resid_pre(model, toks_minus, cfg.layer)
    return (resid_plus - resid_minus).squeeze(0).detach()  # [apos, d_model]


# HOOK + MASK LOGIC
# Injects the steering vector at a specific token span + Creates a mask to exclude tokens directly influenced by injection.
# We don’t want to measure perplexity on tokens we directly manipulated.
def make_actadd_hook_and_span(steering_vec, coeff, prompt_len, addition_location="front"):
    apos = steering_vec.shape[0]
    if apos > prompt_len:
        raise ValueError(f"steering vec length {apos} > prompt_len {prompt_len}")

    if addition_location == "front":
        start = 0
    elif addition_location == "back":
        start = prompt_len - apos
    elif addition_location == "mid":
        start = (prompt_len - apos) // 2
    else:
        raise ValueError("addition_location must be one of: front, mid, back")

    end = start + apos
    vec_sum = (coeff * steering_vec).to(dtype=torch.float32, device=steering_vec.device)


    def hook(resid_pre, hook=None, **kwargs):
        if resid_pre.shape[1] != prompt_len:
            return
        resid_pre[:, start:end, :] += vec_sum

    return hook, (start, end)

# Span is the python tuple, containing two numbers: (start, end)
def target_mask_from_span(seq_len: int, span: Tuple[int, int]) -> torch.Tensor:
    start, end = span
    keep = torch.ones(seq_len - 1, dtype=torch.bool)

    # Token at index 'i' predicts target token at index 'i+1'
    # So in the keep, we will not count score of index i, which will predict i+1
    lo = max(1, start + 1)
    hi = min(seq_len - 1, end)  # inclusive target index
    if lo <= hi:
        keep[lo - 1 : hi] = False
    return keep


# Word counter function, which looks at the paragraph and returns, number of wedding words
# and then total number of words.
def wedding_count_words(text: str) -> Tuple[int, int]:
    words = re.findall(r"\b\w+\b", text.lower())
    matches = WEDDING_RE.findall(text)
    return len(matches), len(words)

def wedding_frequency(model: HookedTransformer, text: str, denom: str = "tokens") -> float:

    k, n_words = wedding_count_words(text)
    if denom == "words":
        return 0.0 if n_words == 0 else k / n_words

    toks = to_tokens(model, text)  # includes BOS; use len-1 as token count for text
    n_toks = max(int(toks.shape[1] - 1), 1)
    return k / n_toks

def is_wedding_related(text: str) -> bool:
    return bool(WEDDING_RE.search(text))


# -----------------------------
# Logprob scoring
# -----------------------------
@torch.no_grad()
def sum_logprobs_for_targets(logits: torch.Tensor, toks: torch.Tensor, keep_mask: torch.Tensor) -> Tuple[float, int]:
    lp = logits[:, :-1, :].log_softmax(dim=-1)
    targets = toks[:, 1:].unsqueeze(-1)
    token_lp = torch.gather(lp, dim=-1, index=targets).squeeze(-1).squeeze(0)  # [seq-1]
    token_lp = token_lp[keep_mask.to(token_lp.device)]
    return float(token_lp.sum().item()), int(token_lp.numel())

@torch.no_grad()
def delta_logprob_for_sentence(model, toks, act_cfg, steering_vec) -> Optional[Tuple[float, int]]:
    """
    Returns (delta = mean_lp_act - mean_lp_base, n_targets_kept)
    Baseline and ActAdd use identical target mask (important). [1](https://shvenergy-my.sharepoint.com/personal/hrudai_mekala_shvenergy_com/Documents/Microsoft%20Copilot%20Chat%20Files/Steering%20language%20models%20with%20activation%20engineering.pdf)
    """
    if toks.shape[1] < 2:
        return None

    prompt_len = toks.shape[1]
    hook_fn, span = make_actadd_hook_and_span(steering_vec, act_cfg.coeff, prompt_len, act_cfg.addition_location)
    keep_mask = target_mask_from_span(prompt_len, span)

    # Baseline
    logits_b = model(toks)
    sum_b, n_b = sum_logprobs_for_targets(logits_b, toks, keep_mask)
    if n_b == 0:
        return None
    mean_b = sum_b / n_b

    # ActAdd
    fwd_hooks = [(f"blocks.{act_cfg.layer}.hook_resid_pre", hook_fn)]
    with model.hooks(fwd_hooks=fwd_hooks):
        logits_a = model(toks)
    sum_a, n_a = sum_logprobs_for_targets(logits_a, toks, keep_mask)
    if n_a == 0:
        return None
    mean_a = sum_a / n_a

    return (mean_a - mean_b, n_b)


@torch.no_grad()
def delta_logprob_for_document(model, text, act_cfg, steering_vec, max_sentences, max_tokens_per_sentence) -> Optional[float]:
    sents = split_into_sentences(text)
    if max_sentences is not None:
        sents = sents[:max_sentences]

    total_delta = 0.0
    total_n = 0

    for s in sents:
        scored = (" " + s) if act_cfg.prefix_space_to_scored_text else s
        toks = to_tokens(model, scored)
        if toks.shape[1] > max_tokens_per_sentence:
            toks = toks[:, :max_tokens_per_sentence]

        out = delta_logprob_for_sentence(model, toks, act_cfg, steering_vec)
        if out is None:
            continue
        delta, n = out
        total_delta += delta * n
        total_n += n

    if total_n == 0:
        return None
    return total_delta / total_n


# -----------------------------
# Bootstrap CI for each bin (diagnose bumps)
# -----------------------------
def bootstrap_ci(x: np.ndarray, n_boot: int = 400, alpha: float = 0.05, rng=None) -> Tuple[float, float]:
    if rng is None:
        rng = np.random.default_rng(0)
    n = len(x)
    if n < 2:
        return (float("nan"), float("nan"))
    means = []
    for _ in range(n_boot):
        samp = rng.choice(x, size=n, replace=True)
        means.append(samp.mean())
    means = np.sort(np.array(means))
    lo = means[int((alpha/2) * n_boot)]
    hi = means[int((1 - alpha/2) * n_boot) - 1]
    return float(lo), float(hi)


# -----------------------------
# Main experiment (Appendix C.1): mean Δlogprob per bin, then exp(-meanΔ) [1](https://shvenergy-my.sharepoint.com/personal/hrudai_mekala_shvenergy_com/Documents/Microsoft%20Copilot%20Chat%20Files/Steering%20language%20models%20with%20activation%20engineering.pdf)
# -----------------------------
def run_experiment(args):
    torch.set_grad_enabled(False)

    dtype = torch.float16 if args.fp16 else torch.float32
    model = HookedTransformer.from_pretrained("gpt2-xl", dtype=dtype)
    model.eval()
    if torch.cuda.is_available() and not args.cpu:
        model.to("cuda")

    act_cfg = ActAddConfig(
        p_plus=args.p_plus,
        p_minus=args.p_minus,
        layer=args.layer,
        coeff=args.coeff,
        addition_location=args.addition_location,
        prefix_space_to_scored_text=not args.no_prefix_space,
    )

    print("Building steering vector...")
    steering_vec = build_actadd_vector(model, act_cfg).to(model.cfg.device)

    print("Loading OpenWebText (streaming)...")
    ds = load_dataset("openwebtext", split="train", streaming=True)
    ds = ds.shuffle(seed=args.seed, buffer_size=args.shuffle_buffer)

    # Sample docs: half wedding-related, half unrelated (paper) [1](https://shvenergy-my.sharepoint.com/personal/hrudai_mekala_shvenergy_com/Documents/Microsoft%20Copilot%20Chat%20Files/Steering%20language%20models%20with%20activation%20engineering.pdf)
    wanted_total = args.n_docs
    wanted_each = wanted_total // 2
    sampled_wed, sampled_non = [], []

    pbar = tqdm(total=wanted_total, desc="Sampling documents", unit="doc")
    for ex in ds:
        text = ex.get("text", "")
        if not text or len(text) < 50:
            continue
        wed = is_wedding_related(text)
        if wed and len(sampled_wed) < wanted_each:
            sampled_wed.append(text); pbar.update(1)
        elif (not wed) and len(sampled_non) < wanted_each:
            sampled_non.append(text); pbar.update(1)
        if len(sampled_wed) >= wanted_each and len(sampled_non) >= wanted_each:
            break
    pbar.close()

    docs = sampled_wed + sampled_non
    rng = np.random.default_rng(args.seed)
    rng.shuffle(docs)

    freqs = []
    deltas = []

    pbar = tqdm(total=len(docs), desc="Scoring Δlogprob", unit="doc")
    for text in docs:
        freq = wedding_frequency(model, text, denom=args.freq_denom)
        delta = delta_logprob_for_document(
            model=model,
            text=text,
            act_cfg=act_cfg,
            steering_vec=steering_vec,
            max_sentences=args.max_sentences,
            max_tokens_per_sentence=args.max_tokens_per_sentence,
        )
        pbar.update(1)
        if delta is None:
            continue
        freqs.append(freq)
        deltas.append(delta)
    pbar.close()

    freqs = np.array(freqs, dtype=np.float64)
    deltas = np.array(deltas, dtype=np.float64)

    # Create bins
    # If you want points to align nicely with ticks, set:
    #   --bin_start 0.0 --bin_width 0.002  => centers at 0.001,0.003,0.005,...
    edges = np.arange(args.bin_start, args.max_freq + args.bin_width, args.bin_width)
    bin_ids = np.digitize(freqs, edges) - 1
    n_bins = len(edges) - 1

    xs, ratios, counts, ci_low, ci_high = [], [], [], [], []

    for b in range(n_bins):
        m = (bin_ids == b)
        count = int(m.sum())
        if count < args.min_bin_count:
            continue

        # X-axis location:
        if args.x_mode == "center":
            lo, hi = edges[b], edges[b + 1]
            x = 0.5 * (lo + hi)
        else:
            # mean frequency of docs in bin (paper-like)
            x = float(freqs[m].mean())

        mean_delta = float(deltas[m].mean())
        ratio = float(np.exp(-mean_delta))  # PPLRatio(b) = exp(-X(b)) [1](https://shvenergy-my.sharepoint.com/personal/hrudai_mekala_shvenergy_com/Documents/Microsoft%20Copilot%20Chat%20Files/Steering%20language%20models%20with%20activation%20engineering.pdf)

        # bootstrap CI on mean_delta then map through exp(-·)
        if args.bootstrap_ci:
            lo_d, hi_d = bootstrap_ci(deltas[m], n_boot=args.n_boot, rng=rng)
            r_lo = float(np.exp(-hi_d))  # note: exp(-x) flips bounds
            r_hi = float(np.exp(-lo_d))
        else:
            r_lo, r_hi = float("nan"), float("nan")

        xs.append(x)
        ratios.append(ratio)
        counts.append(count)
        ci_low.append(r_lo)
        ci_high.append(r_hi)

    xs = np.array(xs)
    ratios = np.array(ratios)
    counts = np.array(counts)
    ci_low = np.array(ci_low)
    ci_high = np.array(ci_high)

    # Sort by x for plotting
    order = np.argsort(xs)
    xs, ratios, counts, ci_low, ci_high = xs[order], ratios[order], counts[order], ci_low[order], ci_high[order]

    os.makedirs(args.out_dir, exist_ok=True)
    fig_path = os.path.join(args.out_dir, "fig2_replication_v2.png")

    plt.figure(figsize=(9, 5))
    plt.plot(xs, ratios, marker="o", linewidth=2, label="Perplexity ratio")

    if args.bootstrap_ci:
        yerr = np.vstack([ratios - ci_low, ci_high - ratios])
        plt.errorbar(xs, ratios, yerr=yerr, fmt="none", capsize=3, alpha=0.6)

    plt.axhline(1.0, linestyle="--", color="gray", linewidth=1)
    plt.xlabel(f"Wedding word frequency (denom={args.freq_denom})")
    plt.ylabel("Perplexity ratio (ActAdd / baseline)")
    plt.title("ActAdd(weddings) perplexity ratio vs wedding-word frequency (v2)")
    plt.grid(True, alpha=0.3)

    # Make ticks “sit under” points if you want that look
    if args.align_ticks_to_points:
        plt.xticks(xs, [f"{v:.4f}" for v in xs], rotation=0)

    if args.annotate_counts:
        for x, y, c in zip(xs, ratios, counts):
            plt.text(x, y, str(int(c)), fontsize=8, ha="center", va="bottom")

    plt.tight_layout()
    plt.savefig(fig_path, dpi=200)
    print(f"\nSaved plot: {fig_path}")

    # Save data for inspection
    np.savez(
        os.path.join(args.out_dir, "fig2_replication_v2_data.npz"),
        xs=xs, ratios=ratios, counts=counts, ci_low=ci_low, ci_high=ci_high,
        freqs=freqs, deltas=deltas, edges=edges
    )
    print(f"Saved data: {os.path.join(args.out_dir, 'fig2_replication_v2_data.npz')}")

    # Print a small table to diagnose bumps
    print("\nBin summary (sorted by x):")
    for x, r, c in zip(xs, ratios, counts):
        print(f"  x={x:.5f}  ratio={r:.6f}  n={int(c)}")


# -----------------------------
# Argparse (Colab/Jupyter-safe)
# -----------------------------
def parse_args(argv=None):
    ap = argparse.ArgumentParser()

    ap.add_argument("--n_docs", type=int, default=2000)
    ap.add_argument("--shuffle_buffer", type=int, default=20000)
    ap.add_argument("--seed", type=int, default=0)

    ap.add_argument("--p_plus", type=str, default=" weddings")
    ap.add_argument("--p_minus", type=str, default=" ")
    ap.add_argument("--layer", type=int, default=16)
    ap.add_argument("--coeff", type=float, default=1.0)
    ap.add_argument("--addition_location", type=str, default="front", choices=["front", "mid", "back"])
    ap.add_argument("--no_prefix_space", action="store_true")

    ap.add_argument("--max_sentences", type=int, default=6)
    ap.add_argument("--max_tokens_per_sentence", type=int, default=192)

    ap.add_argument("--out_dir", type=str, default="actadd_fig2_out")
    ap.add_argument("--cpu", action="store_true")
    ap.add_argument("--fp16", action="store_true")

    # Frequency measurement
    ap.add_argument("--freq_denom", type=str, default="tokens", choices=["tokens", "words"])

    # Binning controls
    ap.add_argument("--bin_start", type=float, default=0.0)
    ap.add_argument("--max_freq", type=float, default=0.03)
    ap.add_argument("--bin_width", type=float, default=0.002)
    ap.add_argument("--min_bin_count", type=int, default=20)

    # X-axis placement and tick alignment
    ap.add_argument("--x_mode", type=str, default="center", choices=["center", "mean"])
    ap.add_argument("--align_ticks_to_points", action="store_true")

    # Diagnostics
    ap.add_argument("--annotate_counts", action="store_true")
    ap.add_argument("--bootstrap_ci", action="store_true")
    ap.add_argument("--n_boot", type=int, default=400)

    args, _unknown = ap.parse_known_args(argv)  # ignore notebook -f kernel.json
    return args


if __name__ == "__main__":
    args = parse_args()
    run_experiment(args)

import os
import re
import argparse
from dataclasses import dataclass
from typing import List, Optional, Tuple

import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

from transformer_lens import HookedTransformer
from datasets import load_dataset

import nltk
from nltk.tokenize import sent_tokenize  # Sentence tokenizer


# WEDDING WORDS
# These are the words, paper recongnises as words related to Weddings

WEDDING_WORDS = [
    "wedding", "weddings", "wed", "marry", "married", "marriage", "bride", "groom", "honeymoon"
]

WEDDING_RE = re.compile(r"\b(" + "|".join(map(re.escape, WEDDING_WORDS)) + r")\b", re.IGNORECASE)

# ActAdd configuration(p+, p-, injection layer, co-efficient, where to inject, prefix behavious)


@dataclass
class ActAddConfig:
    p_plus: str = " weddings"
    p_minus: str = " "
    layer: int = 16
    coeff: float = 1.0
    addition_location: str = "front"  # a=1 in paper’s perplexity experiment
    prefix_space_to_scored_text: bool = True


# NLTK sentence tokenizer
# ensure_sentence_tokenizer(), fallback_sentence_split(), split_into_sentences() will be used
# Splits documents into sentences using NLTK (or regex fallback).

def ensure_sentence_tokenizer():
    try:
        nltk.download('punkt', quiet=True)
    except Exception:
        pass
    try:
        nltk.download("punkt_tab", quiet=True)
    except Exception:
        pass
    try:
        _ = sent_tokenize("Hello world. This is a test.")
        return True
    except Exception:
        return False


PUNKT_OK = ensure_sentence_tokenizer()


def fallback_sentence_split(text: str) -> List[str]:
    sents = re.split(r"(?<=[.!?])\s+", text.strip())
    return [s.strip() for s in sents if s and s.strip()]


def split_into_sentences(text: str) -> List[str]:
    # \x00 represents the Null Byte (ASCII value 0), invisible in normal text editors
    # In web-scraped datasets like OpenWebText (which this code uses), null characters often appear as
    # artifacts of binary data, encoding errors, or failed file conversions.
    # Reason - Many underlying text processing libraries (including parts of Python's NLTK and the C libraries it relies on)
    # interpret the Null Character as the "End of String" marker. Without this line: If a document had 5,000 words but a \x00
    # appeared after word 50, the tokenizer might think the string ended there and ignore the remaining 4,950 words.
    # With this line: The null byte is gone, ensuring the tokenizer processes the full text.
    text = text.replace("\x00", "")
    if PUNKT_OK:
        try:
            sents = sent_tokenize(text)
        except Exception:
            sents = fallback_sentence_split(text)
    else:
        sents = fallback_sentence_split(text)
    return [s.strip() for s in sents if s.strip()]


# TOKENS HELPERS / PADDING
def to_tokens(model: HookedTransformer, text: str) -> torch.Tensor:
    return model.to_tokens(text)


def tlen(model: HookedTransformer, text: str) -> int:
    return model.to_tokens(text).shape[1]


# below function makes sure - len(tokens(p_plus)) == len(tokens(p_minus))
def pad_right_spaces_to_match_tokens(model: HookedTransformer, a: str, b: str) -> Tuple[str, str]:
    # paper notes whitespace padding works well
    la, lb = tlen(model, a), tlen(model, b)
    L = max(la, lb)
    while tlen(model, a) < L:
        a += " "
    while tlen(model, b) < L:
        b += " "
    return a, b


# BUILDING STEERING VECTOR: resid_pre(p+) - resid_pre(p-)

# This extracts the residual stream before attention/MLP at that layer.
@torch.no_grad()
def get_resid_pre(model: HookedTransformer, tokens: torch.Tensor, layer: int) -> torch.Tensor:
    hook_name = f"blocks.{layer}.hook_resid_pre"
    cache, hooks, _ = model.get_caching_hooks(lambda n: n == hook_name)
    with model.hooks(fwd_hooks=hooks):
        _ = model(tokens)
    return cache[hook_name]


@torch.no_grad()
def build_actadd_vector(model: HookedTransformer, cfg: ActAddConfig) -> torch.Tensor:
    p_plus, p_minus = pad_right_spaces_to_match_tokens(model, cfg.p_plus, cfg.p_minus)
    toks_plus = model.to_tokens(p_plus)
    toks_minus = model.to_tokens(p_minus)
    resid_plus = get_resid_pre(model, toks_plus, cfg.layer)
    resid_minus = get_resid_pre(model, toks_minus, cfg.layer)
    return (resid_plus - resid_minus).squeeze(0).detach()  # [apos, d_model]


# HOOK + MASK LOGIC
# Injects the steering vector at a specific token span + Creates a mask to exclude tokens directly influenced by injection.
# We don’t want to measure perplexity on tokens we directly manipulated.
def make_actadd_hook_and_span(steering_vec, coeff, prompt_len, addition_location="front"):
    apos = steering_vec.shape[0]
    if apos > prompt_len:
        raise ValueError(f"steering vec length {apos} > prompt_len {prompt_len}")

    if addition_location == "front":
        start = 0
    elif addition_location == "back":
        start = prompt_len - apos
    elif addition_location == "mid":
        start = (prompt_len - apos) // 2
    else:
        raise ValueError("addition_location must be one of: front, mid, back")

    end = start + apos
    vec_sum = (coeff * steering_vec).to(device=steering_vec.device)

    def hook(resid_pre, hook=None, **kwargs):
        if resid_pre.shape[1] != prompt_len:
            return resid_pre
        # make dtype safe (esp. fp16 model)
        resid_pre[:, start:end, :] += vec_sum.to(dtype=resid_pre.dtype)
        return resid_pre

    return hook, (start, end)


def target_mask_from_span(seq_len: int, span: Tuple[int, int]) -> torch.Tensor:
    # Span is the python tuple, containing two numbers: (start, end)
    start, end = span
    keep = torch.ones(seq_len - 1, dtype=torch.bool)

    # Token at index 'i' predicts target token at index 'i+1'
    # So in the keep, we will not count score of index i, which will predict i+1
    keep[start:end] = False

    return keep


# Word counter function, which looks at the paragraph and returns, number of wedding words
# and then total number of words.
def wedding_count_words(text: str) -> Tuple[int, int]:
    words = re.findall(r"\b\w+\b", text.lower())
    matches = WEDDING_RE.findall(text)
    return len(matches), len(words)


def wedding_frequency(model: HookedTransformer, text: str, denom: str = "tokens") -> float:
    matches, words = wedding_count_words(text)
    if denom == "words":
        return 0.0 if words == 0 else matches / words

    toks = to_tokens(model, text)  # Includes BOS(beginning of sentence token)
    # using len-1 as token count for text
    n_toks = max(1, int(toks.shape[1] - 1))
    return matches / n_toks


def is_wedding_related(text: str) -> bool:
    return bool(WEDDING_RE.search(text))


# Log prob scoring
@torch.no_grad()
def sum_logprobs_for_targets(logits: torch.Tensor, toks: torch.Tensor, keep_mask: torch.Tensor) -> Tuple[float, int]:
    lprobs = logits[:, :-1, :].log_softmax(dim=-1)
    targets = toks[:, 1:].unsqueeze(-1)
    token_logprobs = torch.gather(lprobs, dim=-1, index=targets).squeeze(-1).squeeze(0)
    token_logprobs = token_logprobs[keep_mask.to(token_logprobs.device)]
    return float(token_logprobs.sum().item()), int(token_logprobs.numel())


@torch.no_grad()
def delta_logprob_for_sentence(model, toks, act_cfg, steering_vec) -> Optional[Tuple[float, int]]:
    if toks.shape[1] < 2:
        return None

    prompt_len = toks.shape[1]
    hook_fn, span = make_actadd_hook_and_span(
        steering_vec, act_cfg.coeff, prompt_len, act_cfg.addition_location
    )
    keep_mask = target_mask_from_span(prompt_len, span)

    # Baseline - without ActAdd
    logits_b = model(toks)
    sum_b, n_b = sum_logprobs_for_targets(logits_b, toks, keep_mask)
    if n_b == 0:
        return None
    mean_b = sum_b / n_b

    # With ActAdd - Steered AI
    fwd_hooks = [(f"blocks.{act_cfg.layer}.hook_resid_pre", hook_fn)]
    with model.hooks(fwd_hooks=fwd_hooks):
        logits_a = model(toks)
    sum_a, n_a = sum_logprobs_for_targets(logits_a, toks, keep_mask)
    if n_a == 0:
        return None
    mean_a = sum_a / n_a

    return (mean_a - mean_b, n_b)


@torch.no_grad()
def delta_logprob_for_document(
    model,
    text,
    act_cfg,
    steering_vec,
    max_sentences,
    max_tokens_per_sentence
) -> Optional[float]:
    sentences = split_into_sentences(text)
    if max_sentences is not None:
        sentences = sentences[:max_sentences]
    total_delta = 0.0
    total_n = 0

    for s in sentences:
        scored = (" " + s) if act_cfg.prefix_space_to_scored_text else s
        toks = model.to_tokens(scored)
        if toks.shape[1] > max_tokens_per_sentence:
            toks = toks[:, :max_tokens_per_sentence]

        out = delta_logprob_for_sentence(model, toks, act_cfg, steering_vec)
        if out is None:
            continue
        delta, n = out
        total_delta += delta
        total_n += n

    if total_n == 0:
        return None
    return total_delta / total_n


# Bootstrap CI for each bin (diagnose bumps)
# -----------------------------
def bootstrap_ci(x: np.ndarray, n_boot: int = 400, alpha: float = 0.05, rng=None) -> Tuple[float, float]:
    if rng is None:
        rng = np.random.default_rng(0)
    n = len(x)
    if n < 2:
        return (float("nan"), float("nan"))
    means = []
    for _ in range(n_boot):
        samp = rng.choice(x, size=n, replace=True)
        means.append(samp.mean())
    means = np.sort(np.array(means))
    lo = means[int((alpha / 2) * n_boot)]
    hi = means[int((1 - alpha / 2) * n_boot) - 1]
    return float(lo), float(hi)


def run_experiment(args):
    torch.set_grad_enabled(False)
    dtype = torch.float16 if args.fp16 else torch.float32
    model = HookedTransformer.from_pretrained("gpt2-xl", dtype=dtype)
    model.eval()
    if torch.cuda.is_available() and not args.cpu:
        model.to("cuda")

    act_cfg = ActAddConfig(
        p_plus=args.p_plus,
        p_minus=args.p_minus,
        layer=args.layer,
        coeff=args.coeff,
        addition_location=args.addition_location,
        prefix_space_to_scored_text=not args.no_prefix_space
    )

    print("Building steering vector...")
    steering_vec = build_actadd_vector(model, act_cfg).to(model.cfg.device)

    print("Loading OpenwebText streaming dataset")
    ds = load_dataset("openwebtext", split="train", streaming=True)
    ds = ds.shuffle(seed=args.seed, buffer_size=args.shuffle_buffer)

    wanted_total = args.n_docs
    wanted_each = wanted_total // 2
    sampled_wed, sampled_non = [], []

    pbar = tqdm(total=wanted_total, desc="Sampling documents", unit="doc")

    for ex in ds:
        text = ex.get("text", "")
        if not text or len(text) < 50:
            continue
        wed = is_wedding_related(text)
        if wed and len(sampled_wed) < wanted_each:
            sampled_wed.append(text)
            pbar.update(1)
        elif (not wed) and len(sampled_non) < wanted_each:
            sampled_non.append(text)
            pbar.update(1)
        if len(sampled_wed) >= wanted_each and len(sampled_non) >= wanted_each:
            break
    pbar.close()

    docs = sampled_wed + sampled_non
    rng = np.random.default_rng(args.seed)
    rng.shuffle(docs)

    freqs = []
    deltas = []

    pbar = tqdm(total=len(docs), desc="Scoring Δlogprob", unit="doc")
    for text in docs:
        freq = wedding_frequency(model, text, denom=args.freq_denom)
        delta = delta_logprob_for_document(
            model=model,
            text=text,
            act_cfg=act_cfg,
            steering_vec=steering_vec,
            max_sentences=args.max_sentences,
            max_tokens_per_sentence=args.max_tokens_per_sentence,
        )
        pbar.update(1)
        if delta is None:
            continue
        freqs.append(freq)
        deltas.append(delta)
    pbar.close()

    freqs = np.array(freqs, dtype=np.float64)
    deltas = np.array(deltas, dtype=np.float64)

    # Create bins
    # If you want points to align nicely with ticks, set:
    #   --bin_start 0.0 --bin_width 0.002  => centers at 0.001,0.003,0.005,...
    edges = np.arange(args.bin_start, args.max_freq + args.bin_width, args.bin_width)
    bin_ids = np.digitize(freqs, edges) - 1
    n_bins = len(edges) - 1

    xs, ratios, counts, ci_low, ci_high = [], [], [], [], []

    for b in range(n_bins):
        m = (bin_ids == b)
        count = int(m.sum())
        if count < args.min_bin_count:
            continue

        # X-axis location:
        if args.x_mode == "center":
            lo, hi = edges[b], edges[b + 1]
            x = 0.5 * (lo + hi)
        else:
            # mean frequency of docs in bin (paper-like)
            x = float(freqs[m].mean())

        mean_delta = float(deltas[m].mean())
        ratio = float(np.exp(-mean_delta))  # PPLRatio(b) = exp(-X(b))

        # bootstrap CI on mean_delta then map through exp(-·)
        if args.bootstrap_ci:
            lo_d, hi_d = bootstrap_ci(deltas[m], n_boot=args.n_boot, rng=rng)
            r_lo = float(np.exp(-hi_d))  # note: exp(-x) flips bounds
            r_hi = float(np.exp(-lo_d))
        else:
            r_lo, r_hi = float("nan"), float("nan")

        xs.append(x)
        ratios.append(ratio)
        counts.append(count)
        ci_low.append(r_lo)
        ci_high.append(r_hi)

    xs = np.array(xs)
    ratios = np.array(ratios)
    counts = np.array(counts)
    ci_low = np.array(ci_low)
    ci_high = np.array(ci_high)

    # Sort by x for plotting
    order = np.argsort(xs)
    xs, ratios, counts, ci_low, ci_high = xs[order], ratios[order], counts[order], ci_low[order], ci_high[order]

    os.makedirs(args.out_dir, exist_ok=True)
    fig_path = os.path.join(args.out_dir, "fig2_replication_v2.png")

    plt.figure(figsize=(9, 5))
    plt.plot(xs, ratios, marker="o", linewidth=2, label="Perplexity ratio")

    if args.bootstrap_ci:
        yerr = np.vstack([ratios - ci_low, ci_high - ratios])
        plt.errorbar(xs, ratios, yerr=yerr, fmt="none", capsize=3, alpha=0.6)

    plt.axhline(1.0, linestyle="--", color="gray", linewidth=1)
    plt.xlabel(f"Wedding word frequency (denom={args.freq_denom})")
    plt.ylabel("Perplexity ratio (ActAdd / baseline)")
    plt.title("ActAdd(weddings) perplexity ratio vs wedding-word frequency (v2)")
    plt.grid(True, alpha=0.3)

    # Make ticks “sit under” points if you want that look
    if args.align_ticks_to_points:
        plt.xticks(xs, [f"{v:.4f}" for v in xs], rotation=0)

    if args.annotate_counts:
        for x, y, c in zip(xs, ratios, counts):
            plt.text(x, y, str(int(c)), fontsize=8, ha="center", va="bottom")

    plt.tight_layout()
    plt.savefig(fig_path, dpi=200)
    print(f"\nSaved plot: {fig_path}")

    # Save data for inspection
    np.savez(
        os.path.join(args.out_dir, "fig2_replication_v2_data.npz"),
        xs=xs, ratios=ratios, counts=counts, ci_low=ci_low, ci_high=ci_high,
        freqs=freqs, deltas=deltas, edges=edges
    )
    print(f"Saved data: {os.path.join(args.out_dir, 'fig2_replication_v2_data.npz')}")

    # Print a small table to diagnose bumps
    print("\nBin summary (sorted by x):")
    for x, r, c in zip(xs, ratios, counts):
        print(f"  x={x:.5f}  ratio={r:.6f}  n={int(c)}")


# ARGPARSE
def parse_args(argv=None):
    ap = argparse.ArgumentParser()

    ap.add_argument("--n_docs", type=int, default=2000)
    ap.add_argument("--shuffle_buffer", type=int, default=20000)
    ap.add_argument("--seed", type=int, default=0)

    ap.add_argument("--p_plus", type=str, default=" weddings")
    ap.add_argument("--p_minus", type=str, default=" ")
    ap.add_argument("--layer", type=int, default=16)
    ap.add_argument("--coeff", type=float, default=1.0)
    ap.add_argument("--addition_location", type=str, default="front", choices=["front", "mid", "back"])
    ap.add_argument("--no_prefix_space", action="store_true")

    ap.add_argument("--max_sentences", type=int, default=6)
    ap.add_argument("--max_tokens_per_sentence", type=int, default=192)

    ap.add_argument("--out_dir", type=str, default="actadd_fig2_out")
    ap.add_argument("--cpu", action="store_true")
    ap.add_argument("--fp16", action="store_true")

    # Frequency measurement
    ap.add_argument("--freq_denom", type=str, default="tokens", choices=["tokens", "words"])

    # Binning controls
    ap.add_argument("--bin_start", type=float, default=0.0)
    ap.add_argument("--max_freq", type=float, default=0.03)
    ap.add_argument("--bin_width", type=float, default=0.002)
    ap.add_argument("--min_bin_count", type=int, default=20)

    # X-axis placement and tick alignment
    ap.add_argument("--x_mode", type=str, default="center", choices=["center", "mean"])
    ap.add_argument("--align_ticks_to_points", action="store_true")

    # Diagnostics
    ap.add_argument("--annotate_counts", action="store_true")
    ap.add_argument("--bootstrap_ci", action="store_true")
    ap.add_argument("--n_boot", type=int, default=400)

    args, _unknown = ap.parse_known_args(argv)  # ignore notebook -f kernel.json
    return args


if __name__ == "__main__":
    args = parse_args()
    run_experiment(args)

